{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1122970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g.palacio/IAE/nlp/env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from spacy import displacy  # outil de visualisation intégré de spaCy\n",
    "\n",
    "\n",
    "#from PyPDF2 import PdfReader pour les fichiers pdf\n",
    "\n",
    "\n",
    "# pour la partie sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# pour entraîner un modèle\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f8750",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dd465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/g.palacio/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/g.palacio/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/g.palacio/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  #Télécharge le tokenizer de phrases et de mots. Il permet à nltk de découper un texte en phrases ou en mots\n",
    "\n",
    "\n",
    "nltk.download('stopwords') # Télécharge la liste des stopwords (mots très fréquents comme \"le\", \"et\", \"de\", \"the\", \"is\", etc.). \n",
    "#Ces mots sont souvent supprimés en prétraitement car ils n'apportent pas d'information significative\n",
    "\n",
    "\n",
    "nltk.download('wordnet') #Télécharge WordNet, une base de données lexicales de l’anglais.\n",
    "#Elle est utilisée pour des tâches comme la lemmatisation (trouver la forme de base d’un mot) ou la recherche de synonymes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'This', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Charge le modèle de langue anglais \"en_core_web_sm\" de spaCy\n",
    "\n",
    "doc = nlp(\"Hello! This is a test\")  # Applique le pipeline NLP au texte \"Hello! This is a test\n",
    "\n",
    "print([token.text for token in doc]) #  liste de mots (ou tokens) extraits du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a280e5",
   "metadata": {},
   "source": [
    "Résultat : un objet Doc contenant des tokens (= mots, ponctuation, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876d27e",
   "metadata": {},
   "source": [
    "### Tokenisation  exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.N.A.\n",
      "startup\n",
      "for\n",
      "$\n",
      "6\n",
      "million\n",
      "Tesla PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.N.A. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "6 NUM\n",
      "million NUM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x17b8c36a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x17b8c3a60>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x17bb6a190>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x17bbe6d40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x17bbe9980>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x17b858660>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.N.A. startup for $6 million')  # u (optionnel ici) indique que la chaîne est en Unicode (utile en Python 2, mais pas nécessaire en Python 3)\n",
    "\n",
    "\n",
    "# tokenisation (token.text)\n",
    "for token in doc:\n",
    "    print(token. text)  #Affiche chaque token (mot, ponctuation, symbole) un par un\n",
    "\n",
    "\n",
    "# fonctions grammaticales\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)  # Affiche chaque token avec sa catégorie grammaticale (pos_ = part of speech)\n",
    "\n",
    "\n",
    "\n",
    "nlp.pipeline  #   liste des composants du pipeline NLP chargé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548d0f8",
   "metadata": {},
   "source": [
    "## Exemple displacy\n",
    "\n",
    "Outil de visualisation intégré de spaCy, pour représenter :\n",
    "\n",
    "- les relations grammaticales (avec style='dep') ;\n",
    "\n",
    "- les entités nommées (avec style='ent')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd1ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"58a846eee1114e838a9efd1e892ade59-0\" class=\"displacy\" width=\"1250\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Microsoft</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">plans</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">acquire</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">French</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">AI</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">2025.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-0\" stroke-width=\"2px\" d=\"M70,302.0 C70,242.0 150.0,242.0 150.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,304.0 L62,292.0 78,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-1\" stroke-width=\"2px\" d=\"M310,302.0 C310,242.0 390.0,242.0 390.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,304.0 L302,292.0 318,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-2\" stroke-width=\"2px\" d=\"M190,302.0 C190,182.0 395.0,182.0 395.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,304.0 L403.0,292.0 387.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-3\" stroke-width=\"2px\" d=\"M550,302.0 C550,122.0 880.0,122.0 880.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550,304.0 L542,292.0 558,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-4\" stroke-width=\"2px\" d=\"M670,302.0 C670,182.0 875.0,182.0 875.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,304.0 L662,292.0 678,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-5\" stroke-width=\"2px\" d=\"M790,302.0 C790,242.0 870.0,242.0 870.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,304.0 L782,292.0 798,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-6\" stroke-width=\"2px\" d=\"M430,302.0 C430,62.0 885.0,62.0 885.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M885.0,304.0 L893.0,292.0 877.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-7\" stroke-width=\"2px\" d=\"M430,302.0 C430,2.0 1010.0,2.0 1010.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1010.0,304.0 L1018.0,292.0 1002.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-58a846eee1114e838a9efd1e892ade59-0-8\" stroke-width=\"2px\" d=\"M1030,302.0 C1030,242.0 1110.0,242.0 1110.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-58a846eee1114e838a9efd1e892ade59-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1110.0,304.0 L1118.0,292.0 1102.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dépendances grammaticales (style='dep')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"Microsoft plans to acquire a French AI startup in 2025.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55ed385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " made \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $9.4 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in profit in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Q1 2023\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", largely driven by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AWS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and advertising.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Entités nommées (style='ent')\n",
    "\n",
    "text = \"Amazon made $9.4 billion in profit in Q1 2023, largely driven by AWS and advertising.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c8189",
   "metadata": {},
   "source": [
    "## Pipeline NLP avec spaCy :\n",
    "\n",
    "Le pipeline NLP est une suite d'étapes automatiques appliquées à un texte pour en extraire des informations linguistiques utiles.\n",
    "Voici les principales étapes réalisées par spaCy lors du traitement d'un texte :\n",
    "\n",
    "1. Tokenisation : découpe le texte en unités appelées tokens (mots, ponctuation, etc.)\n",
    "\n",
    "2. POS Tagging : assigne à chaque token sa catégorie grammaticale (nom, verbe, adjectif...)\n",
    "\n",
    "3. Lemmatization : réduit chaque mot à sa forme de base (ex : \"was\" → \"be\")  // Stemming (via NLTK) ( plus simple que la lemmatisation : on coupe la fin des mots)\n",
    "\n",
    "4. Stop Words : identifie les mots fréquents peu informatifs (ex : \"the\", \"and\") que l'on peut ignorer\n",
    "\n",
    "5. Named Entity Recognition (NER) : détecte les entités nommées comme les noms de personnes, dates, lieux, etc\n",
    "6. Sentence Segmentation : découpe le texte en phrases\n",
    "\n",
    "Toutes ces opérations sont automatiquement appliquées dès qu'on passe un texte à nlp()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94243d4",
   "metadata": {},
   "source": [
    "## Exemple pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d157d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "Barack\n",
      "Obama\n",
      "was\n",
      "born\n",
      "in\n",
      "Hawaii\n",
      "in\n",
      "1961\n",
      ".\n",
      "He\n",
      "was\n",
      "elected\n",
      "president\n",
      "in\n",
      "2008\n",
      ".\n",
      "\n",
      "Token - POS - Lemma - Is Stopword:\n",
      "Barack - PROPN - Barack - False\n",
      "Obama - PROPN - Obama - False\n",
      "was - AUX - be - True\n",
      "born - VERB - bear - False\n",
      "in - ADP - in - True\n",
      "Hawaii - PROPN - Hawaii - False\n",
      "in - ADP - in - True\n",
      "1961 - NUM - 1961 - False\n",
      ". - PUNCT - . - False\n",
      "He - PRON - he - True\n",
      "was - AUX - be - True\n",
      "elected - VERB - elect - False\n",
      "president - NOUN - president - False\n",
      "in - ADP - in - True\n",
      "2008 - NUM - 2008 - False\n",
      ". - PUNCT - . - False\n",
      "\n",
      "Entités nommées:\n",
      "Barack Obama (PERSON)\n",
      "Hawaii (GPE)\n",
      "1961 (DATE)\n",
      "2008 (DATE)\n",
      "\n",
      "Phrases:\n",
      "Barack Obama was born in Hawaii in 1961.\n",
      "He was elected president in 2008.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Charger le modèle linguistique anglais\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Exemple de texte\n",
    "text = \"Barack Obama was born in Hawaii in 1961. He was elected president in 2008.\"\n",
    "\n",
    "# Appliquer le pipeline NLP\n",
    "doc = nlp(text)\n",
    "\n",
    "# 1. Tokenisation\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}\")\n",
    "\n",
    "# 2. POS Tagging + Lemmatisation + Stop words\n",
    "print(\"\\nToken - POS - Lemma - Is Stopword:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.pos_} - {token.lemma_} - {token.is_stop}\")\n",
    "\n",
    "# 3. Entités nommées (NER)\n",
    "print(\"\\nEntités nommées:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "# 4. Segmentation en phrases\n",
    "print(\"\\nPhrases:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c7f83",
   "metadata": {},
   "source": [
    "## Gestion des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa9e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ourselves', 'last', 'former', 'until', 'his', 'otherwise', 'one', 'no', 'either', 'never', 'after', 'everyone', 'move', 'somewhere', 'several', 'might', 'regarding', 'seeming', 'front', 'us', 'whenever', 'across', 'give', 'whoever', 'seemed', 'without', 'whereupon', 'others', 'now', 'your', 'have', 'amount', 'everything', 'throughout', 'off', 'none', 'why', 'two', 'often', 'using', 'themselves', 'even', 'name', 'or', 'only', 'did', 'and', 'namely', 'on', '‘ll', 'whereafter', 'they', 'was', 'least', 'may', 'first', 'perhaps', 'all', 'almost', 'whereby', 'much', 'keep', 'while', 'through', 'her', 'via', 'sixty', 'however', 'should', 'whatever', 'where', 'were', 'between', 'four', 'say', 'among', 'he', 'such', 'already', 'nevertheless', 'own', 'are', 'over', 'anyhow', 'below', 'this', 'nowhere', 'thereby', '’ll', 'within', 'because', 'fifteen', 'before', 'alone', 'therefore', 'whether', 'whose', 'does', 'its', 'with', 'well', 'since', 'itself', 'still', 'then', 'full', 'once', 'whither', '’ve', 'n’t', '’m', 'rather', 'anywhere', 'put', 'must', 'more', 'hereby', 'used', 'toward', 'done', 'around', 'someone', 'each', 'what', 'call', 'became', 'formerly', 'but', 'am', 'anyone', 'third', 'these', 'our', 'him', 'forty', 'afterwards', 'thru', 'yet', 'during', 'get', 'yourselves', 'that', 'the', 'any', 'everywhere', 'becoming', 'could', 'is', 'bottom', 'per', 'six', '‘s', 'besides', 'meanwhile', 'side', '‘m', 'himself', 'make', 'had', 'something', 'been', 'anything', 'three', 'again', 'although', 'herein', '‘ve', 'also', 'most', 'how', 'thereupon', 'nine', 'into', 'whence', 'those', 'further', 'here', 'their', 'becomes', 'twenty', 'though', 'empty', 'by', 'many', 'do', 'less', 'somehow', 'enough', 'eight', 'seem', \"'ll\", 'every', 'noone', 'me', 'see', \"'m\", 'few', 'will', 'out', 'please', 'when', 'five', 'sometimes', 'wherein', 'for', 'both', 'in', 'yours', 'i', 'hereafter', 'so', 'about', 'other', 'whole', 'same', 'would', 'of', 'just', 'too', 'thence', 'elsewhere', 'ever', 'if', 'take', 'against', 'behind', 'as', 'neither', 'ours', 'beyond', 'quite', 'made', 'down', 'whom', '’s', '‘re', 'sometime', 'beforehand', 'part', 'go', 'nothing', 'along', 'anyway', \"n't\", 'beside', 'eleven', 'at', 'than', 'hence', 'yourself', 'nor', 'else', 'has', 'there', 'latter', 'myself', 'except', 'it', 'who', 'moreover', \"'ve\", 'upon', \"'re\", 'herself', 'can', 'an', 're', 'top', 'amongst', 'mine', 'onto', 'twelve', 'a', 'ca', 'wherever', 'them', 'fifty', 'hundred', 'from', 'another', 'which', 'always', 'not', '’re', 'my', 'due', 'thereafter', 'to', 'very', 'latterly', 'above', 'together', 'under', 'up', 'cannot', 'therein', 'various', 'hereupon', 'some', 'unless', \"'d\", 'be', '’d', 'back', 'thus', 'show', 'she', 'doing', '‘d', 'serious', 'become', 'being', 'we', 'mostly', \"'s\", 'hers', 'whereas', 'really', 'nobody', 'indeed', 'ten', 'n‘t', 'seems', 'you', 'towards', 'next'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words) # Affiche tous les stop words par défaut de spaCy (en anglais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a44042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vérifie si \"myself\" et \"mystery\" sont considérés comme stop words (True ou False)\n",
    "nlp.vocab['myself'].is_stop\n",
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da57436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajoute \"btw\" (by the way) à la liste des stop words\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3717665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retire \"beyond\" de la liste des stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "nlp.vocab['beyond'].is_stop = False\n",
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8bcc92",
   "metadata": {},
   "source": [
    "### Lemmatisation (forme de base des mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "590c205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t \t I\n",
      "am \t AUX \t \t be\n",
      "a \t DET \t \t a\n",
      "runner \t NOUN \t \t runner\n",
      "running \t VERB \t \t run\n",
      "in \t ADP \t \t in\n",
      "a \t DET \t \t a\n",
      "race \t NOUN \t \t race\n",
      "because \t SCONJ \t \t because\n",
      "I \t PRON \t \t I\n",
      "love \t VERB \t \t love\n",
      "to \t PART \t \t to\n",
      "run \t VERB \t \t run\n",
      "since \t SCONJ \t \t since\n",
      "I \t PRON \t \t I\n",
      "ran \t VERB \t \t run\n",
      "today \t NOUN \t \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\") # on crée un Doc spaCy contenant une phrase avec plusieurs variations du mot run\n",
    "\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', '\\t', token.lemma_) #Affiche chaque mot (token), sa catégorie grammaticale (POS), et son lemme (forme de base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b553d8",
   "metadata": {},
   "source": [
    "Fonction personnalisée pour afficher les lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "265b5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce83481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   I\n",
      "am           AUX    be\n",
      "a            DET    a\n",
      "runner       NOUN   runner\n",
      "running      VERB   run\n",
      "in           ADP    in\n",
      "a            DET    a\n",
      "race         NOUN   race\n",
      "because      SCONJ  because\n",
      "I            PRON   I\n",
      "love         VERB   love\n",
      "to           PART   to\n",
      "run          VERB   run\n",
      "since        SCONJ  since\n",
      "I            PRON   I\n",
      "ran          VERB   run\n",
      "today        NOUN   today\n",
      "easily       ADV    easily\n",
      "and          CCONJ  and\n",
      "fairly       ADV    fairly\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I am a runner running in a race because I love to run since I ran today easily and fairly\")\n",
    "show_lemmas(doc2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55881f2",
   "metadata": {},
   "source": [
    "# Scikit-Learn \n",
    "3 outils servent à transformer du texte brut en vecteurs numériques exploitables par un modèle de Machine Learning\n",
    "\n",
    "\n",
    "### 1. CountVectorizer : \n",
    "\n",
    "Compte le nombre d’occurrences de chaque mot dans chaque document.Produit une matrice document-terme (DTM) avec des comptages simples\n",
    "Limite : ne tient pas compte de la pertinence des mots fréquents dans tous les documents (ex. \"the\", \"is\", etc.)\n",
    "\n",
    "###  Technique TF-IDF (Term Frequency - Inverse Document Frequency) : \n",
    "\n",
    " Pénalise les mots trop fréquents (comme \"the\", \"is\") : Mieux adapté à des textes longs et variés. Elle est intégrée dans Scikit-Learn via :\n",
    "\n",
    "### 2. TfidfTransformer:  \n",
    "\n",
    "Transforme une matrice de comptage (CountVectorizer) en une matrice pondérée TF-IDF.\n",
    "\n",
    "TF-IDF = Term Frequency-Inverse Document Frequency :\n",
    "\n",
    "Accentue les mots rares (plus informatifs) et  réduit le poids des mots très fréquents (peu discriminants)\n",
    "\n",
    "Besoin de deux étapes : fit_transform de CountVectorizer, puis fit_transform de TfidfTransformer\n",
    "\n",
    "### 3. TfidfVectorizer\n",
    "\n",
    "C’est une version combinée des deux précédents.\n",
    "\n",
    "Elle compte les mots ET applique directement la transformation TF-IDF, en une seule étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafc913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annoying' 'be' 'blocked' 'email' 'emails' 'friends' 'from' 'is' 'love'\n",
      " 'should' 'spam']\n",
      "[[1 0 0 1 0 0 0 1 0 0 1]\n",
      " [0 1 1 0 1 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 1 1 0 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNecessaire car le texte est non-structuré, alors que tous les \\nmodèles Scikit-Learn (Naive Bayes, SVM, LogisticRegression, etc.) attendent des tableaux de \\nnombres comme entrée (numpy array, DataFrame, sparse matrix\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avec CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Email spam is annoying\", \"Spam emails should be blocked\", \"I love emails from friends\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "\n",
    "\"\"\"\n",
    "Necessaire car le texte est non-structuré, alors que tous les \n",
    "modèles Scikit-Learn (Naive Bayes, SVM, LogisticRegression, etc.) attendent des tableaux de \n",
    "nombres comme entrée (numpy array, DataFrame, sparse matrix\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use TF-IDF to control for the frequency of words used in the data frame.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also combine the two approaches in a single step \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb1d4b",
   "metadata": {},
   "source": [
    " ## Analyse de sentiment avec VADER (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90662f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "257e5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/g.palacio/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#Let's download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# On télécharge le lexique VADER et on instancie l’analyseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac80be94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.388, 'pos': 0.612, 'compound': 0.8877}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"This was the best, most awesome class EVER!!!\"\n",
    "sid.polarity_scores(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6694aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\n",
    "df = pd.read_csv('amazon.tsv', sep='\\t')\n",
    "\n",
    "df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n",
    "df['compound'] = df['scores'].apply(lambda d: d['compound'])\n",
    "df['comp_score'] = df['compound'].apply(lambda score: 'pos' if score >= 0 else 'neg')\n",
    "\n",
    "\n",
    "Calcule les scores VADER pour chaque review\n",
    "\n",
    "Extrait le score compound\n",
    "\n",
    "Génère une prédiction (comp_score) basée sur ce score\n",
    "\n",
    "\n",
    "\n",
    "accuracy_score(df['label'], df['comp_score'])\n",
    "print(classification_report(df['label'], df['comp_score']))\n",
    "print(confusion_matrix(df['label'], df['comp_score']))\n",
    "\n",
    "\n",
    "On compare les prédictions VADER avec les étiquettes réelles via :\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Rapport de classification (précision, rappel, F1)\n",
    "\n",
    "Matrice de confusion\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec17160",
   "metadata": {},
   "source": [
    "On teste la phrase avec VADER, qui retourne un dictionnaire de scores :\n",
    "\n",
    "pos : score de positivité\n",
    "\n",
    "neg : score de négativité\n",
    "\n",
    "neu : neutralité\n",
    "\n",
    "compound : score global de sentiment (entre -1 et 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddda97",
   "metadata": {},
   "source": [
    "##  Modélisation de sujets avec LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "\n",
    "Découvrir automatiquement les thèmes cachés dans un corpus de documents.\n",
    "\n",
    "Hypothèses : Un document est un mélange de K thèmes latents. Un thème est une distribution de mots.\n",
    "\n",
    "Processus :\n",
    "On choisit le nombre de thèmes.\n",
    "Chaque mot d’un document est associé aléatoirement à un thème.\n",
    "\n",
    "On améliore l'affectation avec des probabilités conditionnelles :\n",
    "\n",
    "P(thème | document)\n",
    "\n",
    "P(mot | thème)\n",
    "\n",
    "On répète l’algorithme jusqu’à stabilisation\n",
    "\n",
    "\n",
    "Dans l'exemple du cours, on utilise LDA (Latent Dirichlet Allocation) pour faire du topic modeling sur des articles de presse Reuters.\n",
    "Le but est de découvrir automatiquement des thématiques dans un corpus non étiqueté (apprentissage non supervisé)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1da931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "reuters = pd.read_csv('Reuters2.csv')\n",
    "\n",
    "\n",
    "#####  Prétraitement et vectorisation\n",
    "cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(reuters['Article'])\n",
    "\n",
    "# On crée une matrice document-terme\n",
    "\n",
    "\n",
    "\n",
    "### Modelisation \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "LDA.fit(dtm)\n",
    "# On ajuste le modèle LDA avec 3 topics. Chaque article devient un mélange de ces topics\n",
    "\n",
    "\n",
    "#### Exploration des topics\n",
    "\n",
    "LDA.components_[0]  # Scores de chaque mot pour le topic 0\n",
    "top_ten_words = single_topic.argsort()[-10:]\n",
    "for index in top_ten_words:\n",
    "    print(cv.get_feature_names_out()[index])\n",
    "## On extrait les mots les plus représentatifs de chaque topic\n",
    "\n",
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{i}')\n",
    "    print([cv.get_feature_names_out()[j] for j in topic.argsort()[-15:]])\n",
    "    #On affiche les 15 mots les plus probables pour chaque topic\n",
    "\n",
    "\n",
    "#### Affectation d’un topic dominant à chaque article\n",
    "\n",
    "topic_results = LDA.transform(dtm)\n",
    "reuters['Topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# On ajoute une colonne Topic qui contient le topic dominant de chaque article"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
